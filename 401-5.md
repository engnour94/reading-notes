# Linked Lists

* Linked Lists:  is a sequence of Nodes that are connected/linked to each other. The most defining feature of a Linked List is that each Node references the next Node in the link.
Linked List - A data structure that contains nodes that links/points to the next node in the list.

#### **types of Linked Lists:**

- Singly: Singly refers to the number of references the node has. A Singly linked list means that there is only one reference, and the reference points to the Next node in a linked list.

- Doubly:
 Doubly refers to there being two (double) references within the node. A Doubly linked list means that there is a reference to both the Next and Previous node.

 - circular :
there is a reference form the last node to the first.

---

#### Terminology

* **Node** : refers to single item in the linked list that holds the data.

* **Head** : refers to the first node in the linked list.

* **Current** : refers to the node currently being looked at. and when we do some operations to the linked * **list** we define the current as the head node to ensure we start at the beginning.

* **Node.next** : reference to the next node. And the final element is not node rather its reference to null.

---
#### What is a Linked List (Part 1)

* Data structures: different ways that we can organize our data
Types of data structures: variables, arrays, hashes, and objects
* Linked lists are linear data structures, which means there is a sequence and an order to how they are constructed and traversed
Non linear data structures: hashes, trees, graphs
Like arrays, in linked lists order matters
* Linked lists don't need to take up a single block of memory, instead, the memory that they use can be scattered (unlike arrays for example, which take up a block of memory all in one place)
* Arrays are static data structures, while linked lists are dynamic data structures
Static data structure: needs all resources to be allocated when the structure is created, will always need a given size of memory even if it's to grow/shrink in size
* Dyamic data structure: can shrink and grow in memory
* Linked list is made up of series of nodes, starting point is the first node which is refered to as the head
* Head is effectively the only entry point to the list and all of its elements
* End of the list isn't a node, but rather a node that point to null or an empty value
* Single node has two parts: data (information the node contains) and a reference to the next node
* A node only knows about what data it contains and who its neighbor is
* This is why a linked list doesn't need a continuous block of memory, a single node has the "address" or a reference to the next node
* Singly linked lists are the simplest type of list, based sonly on fact they go only in one direction
* Doubly linked list: when node can have reference to the subsequent neighbor node and the preceeding node too
* Circular linkeded list: has a node that acts as the tail of the list, and the node after the tail node is the beginning of the list

---
#### What is a Linked List (Part 2)

## Traversal Big O
* The Big O of time for Includes would be ``O(n)``;because, at its worse case, the node we are looking for will be the very last node in the linked list. n represents the number of nodes in the linked list.
* The Big O of space for Includes would be ``O(1)``;This is because there is no additional space being used than what is already given to us with the linked list input.

## Adding a Node O(n)
Adding a node to the middle of a linked list is a bit different than adding to the beginning. This is because we are working with more nodes and must re-allocate to make room for the new node.

## Parts of a linked list
1. **a series of nodes**
2. **head**:starting point of the list is a reference to the first nod 
3. The end of the list isn’t a node, but rather a node that points to null
4. single node is  has just two parts: **data**, or the information that the node contains, and a reference to the next node.

# what even is Big O?
* is a way of evaluating the performance of an algorithm.
* There are two major points to consider when thinking about how an algorithm performs: how much time it requires at runtime given how much time and memory it needs.
*  Big O  express the amount of time that a function, action, or algorithm takes to run based on how many elements we pass to that function.
* That’s exactly what Big O Notation takes into account: the speed and efficiency with which something functions when its input grows to be any (crazy big!) size.

* **types:** 
    1. **O(1) function** takes constant time, which is to say that it doesn’t matter how many elements we have, or how huge our input is: it’ll always take the same amount of time and memory to run our algorithm. 
    2. **O(n) function** is linear, which means that as our input grows (from ten numbers, to ten thousand, to ten million), the space and time that we need to run that algorithm grows linearly.
    3.  **O(n²) function**, which clearly takes exponentially more time and memory the more elements that you have. It’s pretty safe to say that we want to avoid O(n²) algorithms, just from looking at that crazy red line!